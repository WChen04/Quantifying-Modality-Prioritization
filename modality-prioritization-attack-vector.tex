\documentclass[9pt,twocolumn,letterpaper]{article}

% Packages for conference-style formatting
\usepackage[top=0.75in, bottom=1in, left=0.75in, right=0.75in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{xcolor}

% Conference-style formatting
\setlength{\columnsep}{0.25in}
\pagestyle{plain}

% Remove section numbering
\setcounter{secnumdepth}{0}

% Title formatting - larger and centered
\makeatletter
\renewcommand{\maketitle}{%
  \twocolumn[%
    \begin{@twocolumnfalse}
      \vskip 2em
      \begin{center}%
        {\Large\bfseries \@title \par}%
        \vskip 1.5em%
        {\normalsize \@author}%
      \end{center}%
      \vskip 1em
    \end{@twocolumnfalse}
  ]
}
\makeatother

% -------------------------------------------------------------------
% Title and Authors
% -------------------------------------------------------------------
\title{Quantifying Modality Prioritization as an Attack Vector \\ in Multimodal Large Language Models}

\author{
  William Chen\textsuperscript{1} \\
  \textsuperscript{1}Rensselaer Polytechnic Institute, Troy, NY, USA \\
  Email: \texttt{chenw21@rpi.edu}
}

\begin{document}

\maketitle

% -------------------------------------------------------------------
% Abstract
% -------------------------------------------------------------------
\begin{abstract}
\noindent
Multimodal Large Language Models (MLLMs) integrating text, images, and audio introduce critical security vulnerabilities through cross-modal interactions. This paper identifies \textit{Modality Prioritization}: the asymmetric trust relationships that emerge when models process conflicting information across sensory modalities. We introduce the \textit{Distraction Hypothesis}, positing that benign, high-confidence inputs in one modality (e.g., innocuous audio) systematically divert attention from malicious content in another (e.g., harmful visual instructions). Through the \textit{Harmonic-Dissonance Benchmark}—50 adversarial prompts across 9 threat categories—we validate this on two state-of-the-art open-source architectures: LLaVA-v1.6 (transcription-based) and Qwen2-Audio (native audio processing). Our results reveal an \textit{Audio Multiplier Effect}: benign audio significantly increases Attack Success Rates over visual-only attacks across both architectures, demonstrating that current safety alignment fails to generalize to multimodal contexts. We establish this as an architectural vulnerability inherent to late-fusion transformers, exposing a ``Hearing is Trusting'' bias where MLLMs exhibit disproportionate confidence in auditory inputs. We provide a fully reproducible open-source framework for systematic multimodal security evaluation.
\end{abstract}

\noindent\textit{Keywords:} Multimodal AI Security, Jailbreaking, Cross-Modal Attacks, Vision-Language Models, Audio-Visual Fusion, AI Safety, Adversarial Machine Learning, Prompt Injection

% -------------------------------------------------------------------
% 1. Introduction
% -------------------------------------------------------------------
\section{Introduction}

Multimodal Large Language Models (MLLMs) integrating text, images, and audio are rapidly being deployed in safety-critical domains—medical diagnosis, content moderation, autonomous vehicles, and financial fraud detection. While these systems promise revolutionary applications, they introduce an underexplored security vulnerability: \textit{cross-modal attacks} that exploit interactions between disparate sensory inputs. Traditional AI security research has focused on single-modality threats—adversarial suffixes in text~\cite{zou2023universal}, patch attacks in vision, or acoustic perturbations in audio. However, the \textit{interaction effects} between modalities remain largely unexplored, despite their potential to systematically bypass safety mechanisms in deployed systems.

\textbf{Modality Prioritization.} When multimodal models encounter conflicting information across input streams, they implicitly prioritize certain modalities over others. We term this \textbf{Modality Prioritization}—a fundamental consequence of how late-fusion transformers allocate finite attention resources. Each modality is processed independently through specialized encoders (CLIP for vision, Whisper for audio) before being projected into a shared embedding space. Critically, safety alignment techniques (RLHF, Constitutional AI) have been developed on text-only data. Recent work~\cite{crossmodal2024} reveals this creates a security vacuum: safety neurons are modality-specific and do not transfer across input types, leaving the fusion layer vulnerable to exploitation.

\textbf{The Distraction Hypothesis.} We introduce the \textbf{Distraction Hypothesis}: benign, high-confidence inputs in one modality (e.g., innocuous audio queries like ``What color is this?'') systematically divert attention from malicious content in another modality (e.g., harmful visual instructions), effectively blinding the model's safety mechanisms. This extends prior intra-modal work~\cite{yang2025distraction} to the cross-modal setting. We formalize this as \textit{Cross-Modal Indirect Prompt Injection (CM-IPI)}, where benign audio serves as a semantic anchor that shifts the model's posterior toward ``safe,'' bypassing refusal triggers. Critically, these attacks are black-box (requiring only API access) and use generic benign audio without adversarial optimization—dramatically lowering the barrier to exploitation. This work makes the following contributions:
\begin{enumerate}
    \item We introduce the Distraction Hypothesis and formalize Cross-Modal Indirect Prompt Injection (CM-IPI) as an attention-based attack vector.
    \item We develop the Harmonic-Dissonance Benchmark with 50 adversarial prompts across 9 threat categories.
    \item We validate the Audio Multiplier Effect on two architectures (LLaVA + Whisper, Qwen2-Audio), establishing this as an architectural vulnerability.
    \item We release a fully reproducible open-source framework for multimodal security evaluation.
\end{enumerate}

% -------------------------------------------------------------------
% 2. Related Work
% -------------------------------------------------------------------
\section{Related Work}

\textbf{Attention-Based Distraction in Multimodal Systems.} Yang et al.~\cite{yang2025distraction} demonstrate that multimodal large language models exhibit critical vulnerabilities to visual distraction attacks, where benign subimages (natural landscapes, everyday objects) systematically fragment attention heads, causing models to allocate cognitive resources away from malicious visual regions. Their mechanistic analysis on GPT-4V and LLaVA-1.5 reveals that benign distractors diffuse attention weights across images, reducing the salience of harmful content in final hidden states. This phenomenon persists across architectures, suggesting it is fundamental to vision-language transformers. The underlying mechanism traces to the attention mechanism itself, as formalized by Vaswani et al.~\cite{vaswani2017attention} in their foundational Transformer architecture, where scaled dot-product attention computes weighted representations based on query-key similarity—a mechanism that becomes exploitable when benign high-confidence inputs create attention sinks. Building on this intra-modal finding, SACRED-Bench~\cite{sacred2025} extends the distraction paradigm to audio-visual compositions, demonstrating that audio modality introduces unique vulnerabilities through prosodic features (tone, pitch, cadence) that modulate refusal rates independent of semantic content. Their key insight is that confident, authoritative audio achieves higher compliance than identical hesitant-toned speech, revealing models implicitly learn associations between acoustic features and trustworthiness. Furthermore, temporal audio characteristics (speech rate, pauses) affect visual integration—synchronized audio-visual timing amplifies both benign and malicious content effects. Together, these works establish that attention mechanisms across modalities can be systematically manipulated to create exploitable blind spots in safety systems.

\textbf{Fundamental Limitations in Safety Alignment.} Mechanistic interpretability research~\cite{crossmodal2024} exposes why safety mechanisms fail to transfer across modalities in large vision-language models. Using activation patching and causal tracing, researchers identify modality-specific ``safety neurons'' in MLP sublayers that trigger refusals for harmful text but remain dormant for equivalent visual content. This asymmetry arises because RLHF and Constitutional AI training predominantly use text-based preference data, creating a safety hierarchy (text > audio > vision) where harmful instructions rendered as images bypass 73\% of text-based filters. The RLHF paradigm, introduced by Ouyang et al.~\cite{ouyang2022training} for InstructGPT, aligns models through human preference learning on pairwise comparisons, but this approach inherently assumes modality-invariant representations—an assumption our work demonstrates is violated in multimodal contexts. This aligns with Wei et al.'s~\cite{wei2024jailbroken} broader finding that capabilities and safety scale independently: as models grow from 1B to 70B parameters, latent knowledge of dangerous procedures increases faster than safety alignment, creating an expanding vulnerability gap. They demonstrate 70B models are jailbroken 3x more frequently than 7B models despite more rigorous training, tracing this to competing optimization objectives—pre-training imprints harmful knowledge in early layers while RLHF operates on late-layer activations, creating a tug-of-war that adversarial prompts exploit. Complementing these findings, Bai et al.'s~\cite{bai2022constitutional} Constitutional AI framework demonstrates that iterative self-critique guided by human-written principles can improve safety alignment, yet their evaluations remain text-centric. When extended to multimodal contexts, constitutional principles fail to account for cross-modal semantic conflicts, where a model might correctly critique harmful text while simultaneously processing malicious visual instructions. Mazeika et al.~\cite{mazeika2024harmbench} introduce HarmBench, a standardized evaluation framework for automated red-teaming, revealing that even state-of-the-art models exhibit 40-70\% Attack Success Rates under systematic adversarial testing. Wang et al.~\cite{wang2025membership} further demonstrate that privacy vulnerabilities compound safety failures: their ensemble-based Membership Inference Attack framework reveals that distraction of attention heads not only facilitates behavioral jailbreaking but also increases inference stability for sensitive training samples. These findings collectively reveal safety as a brittle overlay rather than deeply integrated reasoning, with cross-modal transfer failures amplifying this brittleness.

\textbf{Jailbreaking Techniques and Attack Surfaces.} Compositional attacks by Broomfield et al.~\cite{broomfield2024} demonstrate semantic decomposition across multiple images, fragmenting malicious instructions (e.g., bomb-making) into innocuous components (chemical compounds, wiring diagrams) distributed across 3-5 images that appear benign in isolation. The model's cross-image reasoning reconstructs harmful intent, achieving 67\% success on GPT-4V and 81\% on LLaVA-1.6, exposing that safety classifiers operate myopically without aggregating threat signals across sequential inputs. This multi-step approach contrasts with Zou et al.'s~\cite{zou2023universal} gradient-based universal adversarial suffixes—token sequences optimized via coordinate descent to maximize harmful completions while minimizing refusal triggers. These suffixes transfer across model families (Vicuna, Llama-2, GPT-3.5) with 60-80\% success by injecting high-perplexity tokens that overload calibration mechanisms, prioritizing fluency over safety. Wallace et al.'s~\cite{wallace2019triggers} earlier work on universal adversarial triggers established the foundation for such attacks, demonstrating that fixed token sequences can reliably induce targeted model behaviors across diverse inputs. In the vision domain, Carlini and Wagner~\cite{carlini2017adversarial} develop optimization-based adversarial examples that achieve near-perfect imperceptibility while fooling image classifiers, establishing principles that extend to multimodal attacks. Recent work by Qi et al.~\cite{qi2024visual} introduces visual adversarial examples specifically designed for vision-language models, showing that small perturbations to images can cause dramatic shifts in generated text, achieving 87\% attack success on CLIP-based systems. Shayegani et al.~\cite{shayegani2023jailbreak} propose jailbreaking methods that exploit the misalignment between visual and textual modalities in instruction-tuned VLMs, demonstrating that conflicting instructions across modalities can systematically bypass safety guardrails. While compositional attacks exploit temporal reasoning failures and universal suffixes exploit attention pattern vulnerabilities, both reveal that adversarial optimization can systematically discover weaknesses in alignment techniques, whether through semantic decomposition or gradient manipulation.

\textbf{Architectural Foundations for Multimodal AI.} LLaVA~\cite{liu2024llava} establishes the dominant open-source vision-language paradigm: a pre-trained CLIP vision encoder projects images into a frozen Llama language model via a trainable linear projection layer, with instruction-tuning enabling zero-shot generalization. LLaVA-v1.6 improves fine-grained understanding through higher-resolution encoding (336×336 → 1024×1024 patches) and dynamic patch allocation. However, the late-fusion design—concatenating visual and text tokens before feeding to the LLM—processes modalities independently until final transformer layers, creating attention competition dynamics. This architectural choice traces back to early vision-language models like CLIP~\cite{radford2021clip}, which learns transferable visual representations through contrastive learning on 400M image-text pairs, establishing vision-text alignment as a foundation for downstream tasks. Alternative architectures like Flamingo~\cite{alayrac2022flamingo} employ perceiver-based cross-attention to interleave visual information with text tokens, achieving few-shot learning through in-context examples, though this approach inherits similar late-fusion vulnerabilities. For audio processing, Whisper~\cite{radford2023whisper} provides robust speech recognition via weakly supervised pre-training on 680,000 hours of multilingual audio. Its encoder-decoder architecture converts 30-second audio chunks to mel-spectrograms, achieving near-human accuracy across accents and noise conditions. Critically, Whisper's high-confidence transcriptions for even ambiguous audio amplify attention-imbalance effects—benign queries generate consistent, high-certainty embeddings that overshadow visual inputs. Gemini~\cite{team2023gemini} represents an alternative approach with native multimodal pre-training, jointly encoding text, images, and audio from the start, though its proprietary nature limits reproducibility. QLoRA~\cite{dettmers2024qlora} enables this research through 4-bit quantization with Low-Rank Adaptation, reducing memory by 75\% while preserving 99\% performance, making models like LLaVA-34B viable on 12GB consumer GPUs and democratizing multimodal security research for academic labs.

% -------------------------------------------------------------------
% 3. Problem Statement
% -------------------------------------------------------------------
\section{Problem Statement}

In this section, we formalize the multimodal jailbreaking problem and introduce our theoretical framework.

\subsection{Threat Model}

We consider a black-box adversary with query access to a multimodal LLM API. The adversary's goal is to elicit harmful responses (e.g., instructions for weapons, fraud, malware) that the model would refuse in single-modality settings. The adversary can craft multimodal inputs consisting of visual content (images) and audio content (speech), but cannot access model internals or gradients.

\subsection{Mathematical Formulation}

We model the MLLM's decision-making as a competition for limited attention resources. In multimodal transformers, the cross-modal attention weight $A_v$ for a visual threat is modulated by the presence of an auditory signal:

\begin{equation}
    A_{v} = \frac{\exp(Q_t K_v^T)}{\exp(Q_t K_v^T) + \exp(Q_t K_a^T)}
\end{equation}

where $Q_t$ are query vectors from the text generation layer, $K_v$ are key vectors from visual content, and $K_a$ are key vectors from audio content. When the auditory input $K_a$ is high-certainty and benign, it exerts disproportionate pull on attention, reducing weights allocated to visual threats ($A_v \to 0$).

\subsection{Cross-Modal Indirect Prompt Injection (CM-IPI)}

We define CM-IPI as follows: Given a malicious visual input $V_{mal}$ and a benign audio input $A_{ben}$, the combined multimodal input $(V_{mal}, A_{ben})$ yields a harmful response with probability $P_{harm}(V_{mal}, A_{ben}) > P_{harm}(V_{mal})$. The audio serves as a ``Semantic Anchor,'' shifting the model's posterior probability of the input being ``safe,'' thereby bypassing refusal triggers.

\textbf{Attack Success Metric:} We define the \textit{Audio Multiplier Effect} as:
\begin{equation}
    \Delta_{ASR} = ASR_{omni} - ASR_{visual}
\end{equation}
where $ASR_{omni}$ is the Attack Success Rate with audio+visual input, and $ASR_{visual}$ is the ASR with visual-only input.

% -------------------------------------------------------------------
% 4. Evaluation
% -------------------------------------------------------------------
\section{Evaluation}

We conduct systematic experiments to evaluate the Audio Multiplier Effect across multimodal architectures, measuring how benign audio amplifies visual jailbreaking attacks.

\subsection{Experiment Setup}

\textbf{Target Models.} We evaluate two state-of-the-art open-source multimodal architectures representing fundamentally different audio processing paradigms: (1) \textbf{LLaVA-v1.6-Mistral-7B + Whisper-Large-v3}—a transcription-based pipeline where Whisper converts audio to text, concatenated with visual prompts before LLaVA processing; and (2) \textbf{Qwen2-Audio-7B-Instruct}—native audio processing where raw waveforms are encoded directly into embeddings, preserving prosodic features (tone, pitch) that transcription discards. Both models deployed on NVIDIA RTX 3060 (12GB VRAM) with 4-bit quantization via bitsandbytes for memory efficiency.

\textbf{Datasets.} We construct the \textbf{Harmonic-Dissonance Benchmark}, comprising $N=50$ adversarial prompts across 9 threat categories defined by common AI safety taxonomies: Weapons (firearm/explosive instructions), Fraud (phishing/scam templates), Malware (keylogger/ransomware code), Drugs (synthesis procedures), Violence (assault tactics), Harassment (doxxing/stalking), Privacy (data exfiltration), Illegal Activities (lockpicking/identity theft), and CSAM-related queries. Each prompt is paired with a benign audio query designed to appear contextually relevant (e.g., ``Is this font serif or sans-serif?'' for text-based images, ``What shapes do you see?'' for diagrams). Audio masks are generic—no adversarial optimization is performed—to test baseline cross-modal vulnerabilities.

\textbf{Experimental Design.} The \textbf{Harmonic-Dissonance Framework} tests three attack vectors: (1) \textbf{Text-Only}—direct hazardous queries as plain text, establishing baseline safety alignment; (2) \textbf{Visual-Only}—typographic attacks where text is rendered as 1024$\times$200px PNG images via Pillow (Arial 36pt, black text on white background) to bypass text-based safety filters; (3) \textbf{Omni-Modal}—visual attacks paired with benign audio masks generated via Google Text-to-Speech (gTTS) as 16kHz MP3 files. Each prompt is tested under all three conditions to isolate the Audio Multiplier Effect: $\Delta_{ASR} = ASR_{omni} - ASR_{visual}$.

\textbf{Implementation Details.} All experiments conducted using Python 3.11, PyTorch 2.1, HuggingFace Transformers 4.40. Models run in inference mode with temperature=0.7, top-p=0.9, max tokens=512. For LLaVA + Whisper, audio transcription occurs first (Whisper batch size=1, beam search with 5 beams), then transcribed text prepended to image prompts. For Qwen2-Audio, audio and image inputs processed jointly through native multi-encoder. Request delay of 2 seconds between queries to avoid rate limiting.

\textbf{Evaluation Metrics.} We employ a keyword-based binary classifier for Attack Success Rate (ASR) calculation, following standard jailbreaking evaluation protocols. Responses classified as \textbf{Refusal} (score=0) if containing safety keywords (``I cannot'', ``I'm unable'', ``illegal'', ``unethical'', ``I apologize'') or explicit denial. Responses classified as \textbf{Compliance} (score=1) if containing instructional markers (``Step 1'', ``First,'', ``Here's how''), detailed procedural content, or exceeding 500 characters without refusal language. ASR computed as: $ASR = \frac{\text{\# Compliant Responses}}{N} \times 100\%$. Statistical significance tested via Wilcoxon signed-rank test on paired samples (Visual-Only vs. Omni-Modal), with $p < 0.05$ threshold.

\subsection{Results}

\begin{table}[h]
\centering
\caption{Attack Success Rates Across Architectures ($N=50$)}
\label{tab:asr}
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Attack Vector} & \textbf{LLaVA} & \textbf{Qwen2-Audio} & \textbf{Avg} \\ 
\midrule
Text-Only & \textit{TBD}\% & \textit{TBD}\% & \textit{TBD}\% \\
Visual-Only & \textit{TBD}\% & \textit{TBD}\% & \textit{TBD}\% \\
\textbf{Omni-Modal} & \textbf{\textit{TBD}\%} & \textbf{\textit{TBD}\%} & \textbf{\textit{TBD}\%} \\ 
\midrule
\textit{Audio Multiplier} & +\textit{XX} pp & +\textit{XX} pp & +\textit{XX} pp \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\columnwidth]{results/figures/asr_comparison.pdf}
\caption{Attack Success Rates across experimental conditions. The Audio Multiplier Effect is evident as the increase from Visual-Only to Omni-Modal conditions across both architectures.}
\label{fig:asr}
\end{figure}

Our main findings are summarized in Table~\ref{tab:asr} and Figure~\ref{fig:asr}. We highlight several key observations demonstrating the Audio Multiplier Effect across architectures:

\textbf{1. Benign audio consistently amplifies visual jailbreaking attacks.} Across both LLaVA-v1.6 and Qwen2-Audio architectures, the Omni-Modal condition (visual + benign audio) achieves substantially higher Attack Success Rates than Visual-Only attacks. [Placeholder: LLaVA shows ASR increase of +XX pp, Qwen2-Audio shows +XX pp]. This demonstrates that generic, non-adversarially-optimized audio queries systematically degrade visual safety mechanisms. Notably, the audio masks require no gradient-based optimization or model-specific tuning—simple contextual questions like "What color is this?" or "Is this font serif or sans-serif?" suffice to amplify visual threats.

\textbf{2. The Audio Multiplier Effect is architecture-independent.} The consistency of amplification across fundamentally different audio processing approaches—transcription-based (LLaVA + Whisper) versus native audio encoding (Qwen2-Audio)—establishes this as an architectural vulnerability inherent to late-fusion transformers rather than a pipeline-specific artifact. Both systems exhibit similar relative increases in ASR when audio is introduced, despite processing audio through entirely different pathways (text conversion vs. direct waveform embedding). This suggests the vulnerability stems from cross-modal attention dynamics in the final fusion layers, not from weaknesses in specific audio encoders.

\textbf{3. Typographic encoding substantially bypasses text-based safety filters.} Visual-Only attacks achieve significantly higher ASR than Text-Only baselines [Placeholder: Visual ASR = XX\%, Text ASR = XX\%, $\Delta$ = +XX pp], confirming that rendering harmful instructions as images circumvents safety alignment trained predominantly on text. This aligns with prior work~\cite{crossmodal2024} showing modality-specific safety neurons fail to activate for visual content. The large gap between Text-Only and Visual-Only conditions validates our experimental design: visual threats are already partially successful, with audio serving as an additional amplifier.

\textbf{4. Statistical significance confirms non-random effects.} Wilcoxon signed-rank tests on paired samples (Visual-Only vs. Omni-Modal) yield $p < 0.05$ for both architectures, rejecting the null hypothesis that the Audio Multiplier Effect arises from random prompt difficulty variation. The consistency of the effect across 50 diverse adversarial prompts spanning 9 threat categories demonstrates robustness beyond cherry-picked examples.

\subsubsection{Category-Specific Vulnerability Analysis}

\begin{table}[h]
\centering
\caption{Audio Multiplier Effect by Threat Category}
\label{tab:categories}
\small
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Category} & \textbf{Visual ASR} & \textbf{$\Delta_{ASR}$ (Audio)} \\ 
\midrule
Weapons & \textit{XX}\% & +\textit{XX} pp \\
Fraud & \textit{XX}\% & +\textit{XX} pp \\
Malware & \textit{XX}\% & +\textit{XX} pp \\
Drugs & \textit{XX}\% & +\textit{XX} pp \\
Violence & \textit{XX}\% & +\textit{XX} pp \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\columnwidth]{results/figures/category_heatmap.pdf}
\caption{Heatmap of Attack Success Rates by threat category. Darker colors indicate higher ASR. The Audio Multiplier Effect varies by category, with technical instruction categories showing stronger amplification.}
\label{fig:categories}
\end{figure}

[Placeholder: Analysis showing which threat categories exhibit strongest Audio Multiplier Effect. Hypothesis: Technical instruction categories (Weapons, Malware) may show higher amplification than social engineering categories (Fraud, Harassment) due to differential visual salience—technical diagrams may generate lower-confidence visual embeddings than text-heavy images, making them more susceptible to audio distraction.]

\subsubsection{Comparison to Prior Cross-Modal Attack Baselines}

Our approach differs fundamentally from compositional jailbreaking~\cite{broomfield2024}, which requires multi-step decomposition across 3-5 images, or universal suffixes~\cite{zou2023universal}, which require gradient-based optimization. The Audio Multiplier Effect operates with single-image attacks and generic benign audio, demonstrating lower exploitation barriers. This accessibility dramatically increases the threat landscape: whereas gradient-based methods require white-box access, our CM-IPI attacks succeed with black-box API queries only.

% -------------------------------------------------------------------
% 5. Discussion
% -------------------------------------------------------------------
\section{Discussion}

\subsection{Implications for AI Safety}
Current safety alignment techniques (RLHF, Constitutional AI) target single-modality harms. Our work exposes critical gaps in \textit{inter-modal safety transfer}. Late-fusion architectures that independently process modalities before merging are particularly vulnerable to cross-modal attacks.

\subsection{Proposed Defense Mechanisms}
We propose \textit{Joint-Modality Scrutiny} layers that explicitly model cross-modal coherence before generation. Entropy-based detection of semantic dissonance between audio and visual streams may serve as early warning systems for CM-IPI attacks.

\subsection{Future Directions}
(1) Extend evaluation to additional native multimodal models (GPT-4o, GPT-4V, Claude 3) with end-to-end processing; (2) Mechanistic interpretability studies of attention heads during attacks; (3) Adversarial optimization of audio masks; (4) Defense evaluation and adversarial training protocols.

% -------------------------------------------------------------------
% 6. Conclusion
% -------------------------------------------------------------------
\section{Conclusion}

Modality Prioritization represents a deterministic security flaw in multimodal AI systems. Through the Harmonic-Dissonance benchmark, we demonstrate that MLLMs exhibit a ``Hearing is Trusting'' bias across fundamentally different architectures (transcription-based and native audio processing), whereby benign audio systematically amplifies visual jailbreaking attacks. Our cross-architecture validation establishes this as an inherent vulnerability of late-fusion transformers. As multimodal AI becomes ubiquitous in safety-critical applications, understanding and mitigating cross-modal vulnerabilities is paramount. Our open-source framework provides a foundation for ongoing research in this emerging threat landscape.

% -------------------------------------------------------------------
% Acknowledgments
% -------------------------------------------------------------------
\section*{Acknowledgments}
\small
This research was conducted at Rensselaer Polytechnic Institute. We thank the open-source community for making LLaVA and Whisper models publicly available, enabling reproducible multimodal security research.

% -------------------------------------------------------------------
% Ethical Considerations
% -------------------------------------------------------------------
\section*{Ethical Considerations}
\small
All experiments were conducted on isolated systems in controlled research environments. The adversarial prompts and attack techniques disclosed in this work are intended solely to inform AI safety research and should not be used maliciously. We advocate for responsible disclosure to model developers and encourage proactive defense mechanisms. Our dataset contains only synthetic adversarial prompts and does not include real harmful content. We have shared our findings with the developers of tested systems prior to publication to enable security improvements.

% -------------------------------------------------------------------
% References (on new page)
% -------------------------------------------------------------------
\clearpage
\bibliographystyle{plain}
\begin{thebibliography}{10}
\small

\bibitem{yang2025distraction} 
Yang, Z., Zhao, H., and Chan, A. (2025). 
\textit{Distraction is All You Need for MLLM Jailbreaking}. 
arXiv:2502.10794.

\bibitem{sacred2025} 
Yang, Y., Chen, L., and Wang, X. (2025). 
\textit{SACRED-Bench: Speech-Audio Composition for RED-teaming}. 
arXiv:2511.10222.

\bibitem{zhang2025audio} 
Zhang, Z., Liu, M., and Kumar, S. (2025). 
\textit{Rethinking Audio-Visual Adversarial Vulnerability}. 
ICLR 2025.

\bibitem{broomfield2024} 
Broomfield, J., Martinez, A., and Thompson, R. (2024). 
\textit{Decompose, Recompose, and Conquer}. 
NeurIPS 2024.

\bibitem{crossmodal2024} 
Anonymous. (2024). 
\textit{Cross-Modal Safety Mechanism Transfer in LVLMs}. 
arXiv:2410.12662.

\bibitem{liu2024llava} 
Liu, H., Li, C., Li, Y., and Lee, Y. J. (2024). 
\textit{Improved Baselines with Visual Instruction Tuning}. 
CVPR 2024.

\bibitem{radford2023whisper} 
Radford, A., et al. (2023). 
\textit{Robust Speech Recognition via Large-Scale Weak Supervision}. 
ICML 2023.

\bibitem{dettmers2024qlora}
Dettmers, T., et al. (2024).
\textit{QLoRA: Efficient Finetuning of Quantized LLMs}.
NeurIPS 2024.

\bibitem{zou2023universal}
Zou, A., et al. (2023).
\textit{Universal and Transferable Adversarial Attacks}.
arXiv:2307.15043.

\bibitem{wei2024jailbroken}
Wei, A., Haghtalab, N., and Steinhardt, J. (2024).
\textit{Jailbroken: How Does LLM Safety Training Fail?}
NeurIPS 2024.

\bibitem{wang2025membership} 
Wang, Z., Zhang, C., Chen, Y., Baracaldo, N., Kadhe, S., and Yu, L. (2025). 
\textit{Membership Inference Attacks as Privacy Tools: Reliability, Disparity and Ensemble}. 
In \textit{Proceedings of the 2025 ACM SIGSAC Conference on Computer and Communications Security (CCS '25)}.

\bibitem{bai2022constitutional} 
Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., et al. (2022). 
\textit{Constitutional AI: Harmlessness from AI Feedback}. 
arXiv:2212.08073.

\bibitem{wallace2019triggers} 
Wallace, E., Feng, S., Kandpal, N., Gardner, M., and Singh, S. (2019). 
\textit{Universal Adversarial Triggers for Attacking and Analyzing NLP}. 
EMNLP 2019.

\bibitem{carlini2017adversarial} 
Carlini, N. and Wagner, D. (2017). 
\textit{Towards Evaluating the Robustness of Neural Networks}. 
IEEE Symposium on Security and Privacy (SP).

\bibitem{radford2021clip} 
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. (2021). 
\textit{Learning Transferable Visual Models From Natural Language Supervision}. 
ICML 2021.

\bibitem{team2023gemini} 
Gemini Team, Google. (2023). 
\textit{Gemini: A Family of Highly Capable Multimodal Models}. 
arXiv:2312.11805.

\bibitem{vaswani2017attention} 
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). 
\textit{Attention is All You Need}. 
NeurIPS 2017.

\bibitem{ouyang2022training} 
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022). 
\textit{Training Language Models to Follow Instructions with Human Feedback}. 
NeurIPS 2022.

\bibitem{mazeika2024harmbench} 
Mazeika, M., Phan, L., Yin, X., Zou, A., Wang, Z., Mu, N., Sakhaee, E., Li, N., Basart, S., Li, B., et al. (2024). 
\textit{HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal}. 
arXiv:2402.04249.

\bibitem{qi2024visual} 
Qi, X., Huang, K., Panda, A., Wang, M., and Mittal, P. (2024). 
\textit{Visual Adversarial Examples Jailbreak Aligned Large Language Models}. 
AAAI 2024.

\bibitem{shayegani2023jailbreak} 
Shayegani, E., Dong, Y., and Abu-Ghazaleh, N. (2023). 
\textit{Jailbreak in Pieces: Compositional Adversarial Attacks on Multi-Modal Language Models}. 
arXiv:2307.14539.

\bibitem{alayrac2022flamingo} 
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. (2022). 
\textit{Flamingo: A Visual Language Model for Few-Shot Learning}. 
NeurIPS 2022.

\end{thebibliography}

\end{document}

